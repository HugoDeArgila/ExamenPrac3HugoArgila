{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bfa09e-f54f-4fa5-bb53-85792757b7f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Preparar el fichero `orders_data.parquet` de modo que pueda ser usado para contruir un 'forecasting model'.  \n",
    "- Limpiar la dataset para que cumpla los requerimientos del equipo de Data y Machine Learning.  \n",
    "- Guardar el archivo actualizado (limpio) como `orders_data_clean.parquet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2991b69f-f0d9-4f7b-8891-71c486e6b7f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  \n",
    "![](https://community.cloud.databricks.com/files/EP_3/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e722f5a-d92b-4210-80ea-6fff061b7081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Como ingeniero de datos de una empresa de comercio electrónico llamada Voltmart, un equipo de aprendizaje automático le ha pedido que limpie los datos que contienen información sobre los pedidos realizados el año pasado. Tienen previsto utilizar estos datos depurados para crear un modelo de previsión de la demanda (Forecasting Model). Para ello, han compartido sus requisitos sobre el formato de tabla de salida deseado.\n",
    "\n",
    "Un analista ha compartido un archivo parquet llamado `orders_data.parquet` para que usted los limpie y los preprocese.\n",
    "\n",
    "A continuación puede ver el esquema del conjunto de datos junto con los requisitos de limpieza de los perezosos analistas de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a793c3b-1d16-4ba8-98b6-1ccb3be7d1b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `orders_data.parquet`\n",
    "\n",
    "| column | data type | description | cleaning requirements | \n",
    "|--------|-----------|-------------|-----------------------|\n",
    "| `order_date` | `timestamp` | Date and time when the order was made | _Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date_ |\n",
    "| `time_of_day` | `string` | Period of the day when the order was made | _New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm_ |\n",
    "| `order_id` | `long` | Order ID | _N/A_ |\n",
    "| `product` | `string` | Name of a product ordered | _Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase_ |\n",
    "| `product_ean` | `double` | Product ID | _N/A_ |\n",
    "| `category` | `string` | Broader category of a product | _Ensure all values are lowercase_ |\n",
    "| `purchase_address` | `string` | Address line where the order was made (\"House Street, City, State Zipcode\") | _N/A_ |\n",
    "| `purchase_state` | `string` | US State of the purchase address | _New column containing: the State that the purchase was ordered from_ |\n",
    "| `quantity_ordered` | `long` | Number of product units ordered | _N/A_ |\n",
    "| `price_each` | `double` | Price of a product unit | _N/A_ |\n",
    "| `cost_price` | `double` | Cost of production per product unit | _N/A_ |\n",
    "| `turnover` | `double` | Total amount paid for a product (quantity x price) | _N/A_ |\n",
    "| `margin` | `double` | Profit made by selling a product (turnover - cost) | _N/A_ |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32eb7dfb-bcca-467e-a058-ffc6a8c57cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    types,\n",
    "    functions as F,\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('cleaning_orders_dataset_with_pyspark')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80163682-819f-4312-b2c9-8c23cba7bd07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('dbfs:/FileStore/shared_uploads/orders_data.parquet')\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83685abc-c333-4fb3-9417-e2cbfc7acba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Respuestas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb9ca43b-a21a-47e2-a533-352c6c8cde6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 1. Modify: Remove orders placed between 12am and 5am (inclusive); convert from timestamp to date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97637f2c-3dcb-467d-b7b9-7e422a02facd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "#Filtramos las filas donde la hora esté entre las 12 AM y 5 AM\n",
    "df_t2 = df_t1.filter(~(F.hour('order_date') >= 0) & (F.hour('order_date') <= 5))\n",
    "\n",
    "\n",
    "#Convertimos la columna order_date de timestamp a date\n",
    "df_t2 = df_t1.withColumn(\"order_date\", F.to_date(df[\"order_date\"]))\n",
    "\n",
    "#Mostramos los cambios\n",
    "df_t2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af688b39-614f-4620-bd5a-b7bae774bc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 2. New column containing (lower bound inclusive, upper bound exclusive): \"morning\" for orders placed 5-12am, \"afternoon\" for orders placed 12-6pm, and \"evening\" for 6-12pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375d45cc-5fa2-41ae-adc5-3d1237dc112f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Crear la nueva columna \"time_of_day\" basándose en la hora de \"order_date\"\n",
    "df_t1 = df.withColumn(\n",
    "    'time_of_day',\n",
    "    F.when(F.hour('order_date').between(5, 11), 'morning')  # De 5 AM a 11:59 AM\n",
    "     .when(F.hour('order_date').between(12, 17), 'afternoon')  # De 12 PM a 5:59 PM\n",
    "     .otherwise('evening')  # De 6 PM a 4:59 AM (incluso la medianoche)\n",
    ")\n",
    "\n",
    "df_t1.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de882743-3a8e-48e5-81b6-b18b033da1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 3. Remove rows containing \"TV\" as the company has stopped selling this product; ensure all values are lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "645805b4-396f-4dd5-8238-3081929226fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Convertimos los valores de la columna \"products\" a minúsculas\n",
    "df_t3 = df_t2.withColumn(\"product\", F.lower(\"product\"))\n",
    "\n",
    "#Eliminamos los valores tv en la columna product\n",
    "df_t3 = df_t3.filter(~df_t3[\"product\"].contains(\"tv\"))\n",
    "\n",
    "df_t3.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00f3cc87-4b9d-4a81-b3d6-e2a0f0af1fe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4. Ensure all values are lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e49653-65e4-445e-995f-9c67489a2248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "#Convertimos los valores de la columna \"category\" a minúsculas\n",
    "df_t4 = df_t3.withColumn(\"category\", F.lower(\"category\"))\n",
    "\n",
    "df_t4.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e15d85b-d5e3-474e-9d85-8a200d10b7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 5. New column containing: the State that the purchase was ordered from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5ba4d8-7ea3-4460-8f9d-218e7a907231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Dividimos la columna 'purchase_address' por la coma\n",
    "df_t5 = df_t4.withColumn(\"purchase_state\", F.split(F.col(\"purchase_address\"), \",\").getItem(2))\n",
    "\n",
    "# Limpiamos espacios extra con trim() para asegurarnos de que no haya espacios al principio o al final\n",
    "df_t5 = df_t5.withColumn(\"purchase_state\", F.trim(F.col(\"purchase_state\")))\n",
    "\n",
    "# Ahora, extraemos el estado (que es la primera parte de la tercera columna, antes del código postal) usando 'split' nuevamente\n",
    "df_t5 = df_t5.withColumn(\"purchase_state\", F.split(F.col(\"purchase_state\"), \" \").getItem(0))\n",
    "\n",
    "df_t5.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1bb2c3-6150-4ecc-b4cc-15047f9c680d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 6. Guardar archivo final limpio con nombre `orders_data_clean.parquet` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "592496fb-b1b2-4bc5-b901-ad9c2841f6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Guardamos el DataFrame como un solo archivo Parquet\n",
    "output_path = \"/FileStore/shared_uploads/orders_data_clean.parquet\"\n",
    "\n",
    "df_t5.coalesce(1).write.mode(\"overwrite\").parquet(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5eacdae-eb35-432d-b0b2-700fe2fd44f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7. Exportar archivo limpio en formato CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b0e3ad-09b8-4426-89f7-17bc337b73aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ruta donde se guardará el archivo CSV\n",
    "csv_output_path = \"/FileStore/shared_uploads/orders_data_clean.csv\"\n",
    "\n",
    "# Leemos el archivo Parquet\n",
    "df_parquet = spark.read.parquet(\"/FileStore/shared_uploads/orders_data_clean.parquet\")\n",
    "\n",
    "# Guardamos el DataFrame como un solo archivo CSV\n",
    "df_parquet.repartition(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(csv_output_path)\n",
    "\n",
    "df_t5.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
